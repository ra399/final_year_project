"""
Data Generation for Surrogate ML Model Training
================================================
Author: BTP Student, Department of Chemical Engineering, IIT Patna
Project: XAI-MPC for Energy-Efficient Distillation Column Operation

Description:
-----------
This script runs closed-loop simulations of the distillation column with MPC
to generate training data for the surrogate machine learning model. The data
includes:
- State variables (compositions, temperatures, flows)
- Control actions (reflux, reboiler duty)
- Disturbances (feed rate, composition, temperature)
- Performance metrics (energy consumption, purity deviation)

The generated data will be used to train Random Forest and XGBoost models
that approximate the MPC control policy.

Output:
-------
- CSV file with 10,000+ state-action pairs
- Feature importance analysis
- Data quality metrics
"""

import numpy as np
import pandas as pd
from typing import Dict, List
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


class MPCDataGenerator:
    """
    Generate training data by running MPC-controlled simulations
    under various operating conditions.
    """
    
    def __init__(self, column_model, mpc_controller):
        """
        Initialize data generator.
        
        Parameters:
        -----------
        column_model : BinaryDistillationColumn
            Distillation column model
        mpc_controller : SimplifiedMPC
            MPC controller
        """
        self.column = column_model
        self.mpc = mpc_controller
        self.data = []
        
    def generate_scenarios(self, n_scenarios: int = 100) -> List[Dict]:
        """
        Generate diverse operating scenarios for data collection.
        
        Scenarios include:
        - Setpoint changes
        - Feed disturbances
        - Different initial conditions
        
        Parameters:
        -----------
        n_scenarios : int
            Number of different scenarios to simulate
        
        Returns:
        --------
        scenarios : list
            List of scenario dictionaries
        """
        scenarios = []
        
        print(f"Generating {n_scenarios} operating scenarios...")
        
        for i in range(n_scenarios):
            # Random setpoint (90-98% ethanol purity)
            x_top_sp = np.random.uniform(0.90, 0.98)
            
            # Random feed conditions
            F = np.random.uniform(80, 120)  # Feed rate: 80-120 kmol/min
            x_F = np.random.uniform(0.30, 0.50)  # Feed composition: 30-50%
            T_F = np.random.uniform(330, 350)  # Feed temperature: 330-350 K
            
            # Random initial control
            L_R_init = np.random.uniform(40, 60)
            Q_R_init = np.random.uniform(4000, 6000)
            
            # Disturbance profile (step changes during simulation)
            disturbance_time = np.random.uniform(20, 60)  # Time of disturbance
            F_disturb = np.random.uniform(-10, 10)  # Feed rate change
            x_F_disturb = np.random.uniform(-0.05, 0.05)  # Composition change
            
            scenario = {
                'x_top_sp': x_top_sp,
                'F_init': F,
                'x_F_init': x_F,
                'T_F': T_F,
                'L_R_init': L_R_init,
                'Q_R_init': Q_R_init,
                'disturbance_time': disturbance_time,
                'F_disturb': F_disturb,
                'x_F_disturb': x_F_disturb
            }
            
            scenarios.append(scenario)
        
        return scenarios
    
    def run_simulation(self, 
                      scenario: Dict,
                      sim_time: float = 100.0,
                      dt: float = 1.0) -> pd.DataFrame:
        """
        Run a single closed-loop simulation with MPC.
        
        Parameters:
        -----------
        scenario : dict
            Operating scenario parameters
        sim_time : float
            Simulation duration (minutes)
        dt : float
            Sampling/control interval (minutes)
        
        Returns:
        --------
        data : pd.DataFrame
            Time-series data from simulation
        """
        # Initialize
        time_steps = int(sim_time / dt)
        
        # Initial steady state
        u_init = {'L_R': scenario['L_R_init'], 'Q_R': scenario['Q_R_init']}
        d_init = {'F': scenario['F_init'], 'x_F': scenario['x_F_init'], 'T_F': scenario['T_F']}
        
        # Get initial state
        M0 = np.full(self.column.n_trays, self.column.M_hold)
        x0 = np.linspace(0.05, scenario['x_top_sp'], self.column.n_trays)
        state = np.concatenate([M0, x0])
        
        # Storage for time series
        data_list = []
        
        # Control inputs
        u_current = u_init.copy()
        d_current = d_init.copy()
        
        # Setpoint
        setpoint = {'x_top': scenario['x_top_sp']}
        
        # Simulation loop
        for k in range(time_steps):
            t = k * dt
            
            # Apply disturbance at specified time
            if t >= scenario['disturbance_time'] and t < scenario['disturbance_time'] + 1.0:
                d_current['F'] += scenario['F_disturb']
                d_current['x_F'] += scenario['x_F_disturb']
                d_current['F'] = np.clip(d_current['F'], 50, 150)
                d_current['x_F'] = np.clip(d_current['x_F'], 0.20, 0.60)
            
            # Extract current outputs
            x = state[self.column.n_trays:2*self.column.n_trays]
            x_top = x[-1]
            x_bottom = x[0]
            
            current_output = {'x_top': x_top, 'x_bottom': x_bottom}
            
            # Compute MPC control action
            u_current = self.mpc.compute_control(
                current_output, setpoint, u_current, d_current
            )
            
            # Simulate one time step
            t_span = (t, t + dt)
            _, states = self.column.simulate(
                t_span, state, u_current, d_current, n_points=2
            )
            state = states[-1, :]
            
            # Store data
            data_point = {
                'time': t,
                'x_top': x_top,
                'x_bottom': x_bottom,
                'x_top_sp': setpoint['x_top'],
                'F': d_current['F'],
                'x_F': d_current['x_F'],
                'T_F': d_current['T_F'],
                'L_R': u_current['L_R'],
                'Q_R': u_current['Q_R'],
                'energy': u_current['Q_R'],  # Energy metric
                'purity_error': abs(x_top - setpoint['x_top'])
            }
            
            data_list.append(data_point)
        
        return pd.DataFrame(data_list)
    
    def generate_dataset(self,
                        n_scenarios: int = 50,
                        sim_time: float = 100.0,
                        save_path: str = 'mpc_training_data.csv') -> pd.DataFrame:
        """
        Generate complete training dataset.
        
        Parameters:
        -----------
        n_scenarios : int
            Number of scenarios to simulate
        sim_time : float
            Duration of each simulation (minutes)
        save_path : str
            Path to save CSV file
        
        Returns:
        --------
        dataset : pd.DataFrame
            Complete training dataset
        """
        print("\n" + "="*70)
        print("MPC Data Generation for ML Training")
        print("="*70)
        
        scenarios = self.generate_scenarios(n_scenarios)
        
        all_data = []
        
        print(f"\nRunning {n_scenarios} closed-loop simulations...")
        print(f"Simulation time: {sim_time} minutes each")
        
        for i, scenario in enumerate(tqdm(scenarios, desc="Simulations")):
            try:
                sim_data = self.run_simulation(scenario, sim_time)
                all_data.append(sim_data)
            except Exception as e:
                print(f"\nâš  Warning: Scenario {i} failed: {str(e)}")
                continue
        
        # Combine all data
        dataset = pd.concat(all_data, ignore_index=True)
        
        print(f"\nâœ“ Data generation complete!")
        print(f"  Total samples: {len(dataset)}")
        print(f"  Features: {dataset.shape[1]}")
        
        # Add derived features
        print(f"\nComputing derived features...")
        dataset['error_integral'] = dataset.groupby(
            dataset.index // 100
        )['purity_error'].cumsum()
        
        dataset['reflux_ratio'] = dataset['L_R'] / (dataset['L_R'] + 50.0)
        dataset['energy_efficiency'] = dataset['Q_R'] / (dataset['x_top'] * 100)
        
        # Save to CSV
        dataset.to_csv(save_path, index=False)
        print(f"âœ“ Dataset saved to: {save_path}")
        
        # Data quality analysis
        self.analyze_dataset(dataset)
        
        return dataset
    
    def analyze_dataset(self, dataset: pd.DataFrame):
        """
        Analyze dataset quality and statistics.
        
        Parameters:
        -----------
        dataset : pd.DataFrame
            Training dataset
        """
        print("\n" + "="*70)
        print("Dataset Analysis")
        print("="*70)
        
        print(f"\nData Statistics:")
        print(dataset.describe().round(3))
        
        print(f"\nFeature Ranges:")
        for col in ['x_top', 'x_F', 'F', 'L_R', 'Q_R']:
            print(f"  {col}: [{dataset[col].min():.3f}, {dataset[col].max():.3f}]")
        
        print(f"\nControl Action Statistics:")
        print(f"  Reflux (L_R):")
        print(f"    Mean: {dataset['L_R'].mean():.2f} kmol/min")
        print(f"    Std: {dataset['L_R'].std():.2f}")
        
        print(f"  Reboiler Duty (Q_R):")
        print(f"    Mean: {dataset['Q_R'].mean():.1f} kJ/min")
        print(f"    Std: {dataset['Q_R'].std():.1f}")
        
        print(f"\nPerformance Metrics:")
        print(f"  Average purity error: {dataset['purity_error'].mean()*100:.3f}%")
        print(f"  Average energy: {dataset['energy'].mean():.1f} kJ/min")
        
        # Check for missing values
        missing = dataset.isnull().sum()
        if missing.any():
            print(f"\nâš  Missing values detected:")
            print(missing[missing > 0])
        else:
            print(f"\nâœ“ No missing values")
        
        print("="*70)


# Example usage and testing
if __name__ == "__main__":
    # Import models (assuming they're in the same directory)
    import sys
    # You would normally import like this:
    # from distillation_model import BinaryDistillationColumn
    # from mpc_controller import SimplifiedMPC
    
    print("="*70)
    print("MPC Data Generation - Standalone Test")
    print("="*70)
    
    # For demonstration, we'll create a synthetic dataset
    # In practice, you would use the actual models
    
    print("\nðŸ“Š Generating synthetic training data...")
    
    # Synthetic data generation (placeholder)
    n_samples = 5000
    
    np.random.seed(42)
    
    data = {
        'time': np.arange(n_samples),
        'x_top': np.random.uniform(0.88, 0.98, n_samples),
        'x_bottom': np.random.uniform(0.02, 0.08, n_samples),
        'x_top_sp': np.random.choice([0.90, 0.93, 0.95, 0.97], n_samples),
        'F': np.random.uniform(80, 120, n_samples),
        'x_F': np.random.uniform(0.35, 0.45, n_samples),
        'T_F': np.random.uniform(335, 345, n_samples),
        'L_R': np.random.uniform(40, 70, n_samples),
        'Q_R': np.random.uniform(4000, 7000, n_samples),
        'energy': np.random.uniform(4000, 7000, n_samples),
        'purity_error': np.random.uniform(0, 0.05, n_samples)
    }
    
    dataset = pd.DataFrame(data)
    
    # Add correlations to make it realistic
    dataset['L_R'] = 45 + 150 * (dataset['x_top_sp'] - dataset['x_top']) + \
                     0.2 * (dataset['F'] - 100) + np.random.normal(0, 2, n_samples)
    
    dataset['Q_R'] = 4500 + 40000 * (dataset['x_top_sp'] - dataset['x_top']) + \
                     20 * (dataset['F'] - 100) + np.random.normal(0, 200, n_samples)
    
    dataset['L_R'] = np.clip(dataset['L_R'], 30, 80)
    dataset['Q_R'] = np.clip(dataset['Q_R'], 3000, 8000)
    
    # Save
    dataset.to_csv('mpc_training_data.csv', index=False)
    
    print(f"âœ“ Generated {len(dataset)} samples")
    print(f"âœ“ Saved to: mpc_training_data.csv")
    
    print(f"\nDataset Preview:")
    print(dataset.head(10))
    
    print(f"\nSummary Statistics:")
    print(dataset[['x_top', 'L_R', 'Q_R']].describe())
    
    print(f"\nâœ“ Data generation complete!")
    print(f"âœ“ Ready for ML model training (Phase 4)")
    print("="*70)